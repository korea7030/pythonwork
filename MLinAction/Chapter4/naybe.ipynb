{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 텍스트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트로 단어 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import * \n",
    "\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                  ['maybe', 'not', 'take', 'him','to','dog','park','stupid'],\n",
    "                  ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless', 'garbage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] # 1: 폭력적인, 0: 폭력적이지 않음\n",
    "    return postingList, classVec\n",
    "\n",
    "def createVocaList(dataSet): # 유일한 단어목록 생성\n",
    "    vocabSet = set([]) # 빈 set 생성\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # vocaSet에 단어 붙이기\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet): # 어휘 목록에 있는 단어가 존재하는지 확인\n",
    "    returnVec = [0]*len(vocabList) # 단어 개수만큼 0으로 vector 생성\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1 # 단어가 어휘목록에 있으면 1\n",
    "        else: print \"the word : %s is not in my Vocabulary!\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n"
     ]
    }
   ],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocaList(listOPosts)\n",
    "print myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print setOfWords2Vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 벡터로 확률 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    \n",
    "    p1Vect = log(p1Num / p1Denom) # log() 로 변경\n",
    "    p0Vect = log(p0Num / p0Denom) # log() 로 변경\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myVocabList = createVocaList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n"
     ]
    }
   ],
   "source": [
    "print len(trainMat); print listOPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb # 폭력적인 단어가 있는 문서의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "       -2.15948425, -3.25809654, -3.25809654, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -3.25809654, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -2.56494936, -1.87180218])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 조건반영을 위한 분류기 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify*p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify*p0Vec) + log(1.0-pClass1)\n",
    "    \n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocaList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat), array(listClasses))\n",
    "    testEntry = ['love','my','dalmation'] # test data\n",
    "    \n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) # test 결과\n",
    "    print thisDoc\n",
    "    print testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    testEntry = ['stupid', 'garbage'] # test data\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print thisDoc\n",
    "    print testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "['love', 'my', 'dalmation'] classified as :  0\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "['stupid', 'garbage'] classified as :  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 단어 문서 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스팸 이메일 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mySent = \"This book is the best book on Python or M.L. I have ever laid eyes upon.\"\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "\n",
    "[tok.lower() for tok in listOfTokens if len(tok) > 0] # 소문자로 나타내면서 길이가 0 보다큰 단어만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emailText = open('email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText = []\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        \n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    vocabList = createVocaList(docList)\n",
    "    trainingSet = range(50); testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print \"the error rate is : \", float(errorCount) / len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is :  0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개인광고에 포함된 지역 특색 도출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 자주 발생하는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]       \n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocaList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.5\n"
     ]
    }
   ],
   "source": [
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "vocabList, pSF, pNY = localWords(ny,sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석 : 지역적으로 사용되는 단어 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny,sf)\n",
    "    topNY = []; topSF = []\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0: topSF.append((vocabList[i], p0V[i]))\n",
    "        if p1V[i] > -6.0: topNY.append((vocabList[i], p1V[i]))\n",
    "    \n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\"\n",
    "    for item in sortedNY:\n",
    "        print item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.35\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "guy\n",
      "hello\n",
      "some\n",
      "nice\n",
      "friends\n",
      "enjoy\n",
      "love\n",
      "work\n",
      "over\n",
      "not\n",
      "don\n",
      "reading\n",
      "job\n",
      "come\n",
      "decent\n",
      "fun\n",
      "off\n",
      "other\n",
      "female\n",
      "time\n",
      "having\n",
      "here\n",
      "others\n",
      "great\n",
      "club\n",
      "join\n",
      "cuddle\n",
      "male\n",
      "want\n",
      "years\n",
      "before\n",
      "our\n",
      "first\n",
      "little\n",
      "white\n",
      "queer\n",
      "earth\n",
      "find\n",
      "please\n",
      "email\n",
      "however\n",
      "been\n",
      "women\n",
      "hoping\n",
      "partner\n",
      "person\n",
      "hiking\n",
      "bit\n",
      "walk\n",
      "night\n",
      "people\n",
      "santa\n",
      "working\n",
      "ladies\n",
      "down\n",
      "cultures\n",
      "but\n",
      "single\n",
      "chat\n",
      "book\n",
      "really\n",
      "all\n",
      "chinese\n",
      "month\n",
      "children\n",
      "send\n",
      "mistakes\n",
      "include\n",
      "woman\n",
      "mainstream\n",
      "very\n",
      "putting\n",
      "asian\n",
      "video\n",
      "link\n",
      "odd\n",
      "even\n",
      "what\n",
      "giving\n",
      "deeply\n",
      "indian\n",
      "learned\n",
      "full\n",
      "never\n",
      "appeara\n",
      "met\n",
      "along\n",
      "healthy\n",
      "shift\n",
      "social\n",
      "usually\n",
      "rafael\n",
      "members\n",
      "family\n",
      "clara\n",
      "county\n",
      "ask\n",
      "two\n",
      "live\n",
      "symphony\n",
      "more\n",
      "company\n",
      "insights\n",
      "hour\n",
      "movies\n",
      "learn\n",
      "dirnks\n",
      "give\n",
      "share\n",
      "hilton\n",
      "pond\n",
      "how\n",
      "sunday\n",
      "occasionally\n",
      "intelligent\n",
      "staying\n",
      "funny\n",
      "light\n",
      "responsibility\n",
      "pleasure\n",
      "talk\n",
      "minded\n",
      "exclusive\n",
      "disabled\n",
      "committed\n",
      "hell\n",
      "still\n",
      "perfect\n",
      "anonymously\n",
      "fit\n",
      "late\n",
      "willing\n",
      "them\n",
      "seeking\n",
      "auto\n",
      "practice\n",
      "veggie\n",
      "lunch\n",
      "name\n",
      "opera\n",
      "raised\n",
      "everyone\n",
      "house\n",
      "fish\n",
      "year\n",
      "sexual\n",
      "saturday\n",
      "special\n",
      "troll\n",
      "living\n",
      "farm\n",
      "seriously\n",
      "got\n",
      "guess\n",
      "california\n",
      "meetups\n",
      "days\n",
      "keep\n",
      "conversation\n",
      "american\n",
      "moving\n",
      "relate\n",
      "dp_patron\n",
      "number\n",
      "one\n",
      "lol\n",
      "another\n",
      "open\n",
      "anyone\n",
      "interests\n",
      "sebastopol\n",
      "way\n",
      "relationship\n",
      "than\n",
      "remember\n",
      "project\n",
      "san\n",
      "well\n",
      "nonfiction\n",
      "need\n",
      "aged\n",
      "chatting\n",
      "able\n",
      "green\n",
      "reach\n",
      "embarcadero\n",
      "experiences\n",
      "plan\n",
      "significant\n",
      "elledge\n",
      "nothing\n",
      "plays\n",
      "sometimes\n",
      "dog\n",
      "clean\n",
      "professional\n",
      "text\n",
      "rosa\n",
      "therapist\n",
      "pick\n",
      "advanced\n",
      "writer\n",
      "being\n",
      "employed\n",
      "interested\n",
      "hope\n",
      "hopefully\n",
      "theft\n",
      "considerate\n",
      "wiccan\n",
      "sustainably\n",
      "seldom\n",
      "married\n",
      "bay\n",
      "jokes\n",
      "discu\n",
      "hike\n",
      "incarcerated\n",
      "sex\n",
      "culture\n",
      "best\n",
      "currently\n",
      "enough\n",
      "probably\n",
      "enjoys\n",
      "recently\n",
      "attention\n",
      "preferably\n",
      "both\n",
      "barely\n",
      "games\n",
      "lookin\n",
      "active\n",
      "due\n",
      "much\n",
      "woke\n",
      "board\n",
      "life\n",
      "educated\n",
      "gal\n",
      "straight\n",
      "chinatown\n",
      "will\n",
      "involved\n",
      "ive\n",
      "site\n",
      "middle\n",
      "cant\n",
      "develop\n",
      "make\n",
      "fidi\n",
      "grand\n",
      "party\n",
      "8am\n",
      "meets\n",
      "used\n",
      "pediatric\n",
      "banter\n",
      "kik\n",
      "heartbroken\n",
      "24yr\n",
      "chickens\n",
      "money\n",
      "thanks\n",
      "yet\n",
      "also\n",
      "drive\n",
      "east\n",
      "easter\n",
      "around\n",
      "know\n",
      "wanting\n",
      "lady\n",
      "rotate\n",
      "pals\n",
      "born\n",
      "clubs\n",
      "home\n",
      "everything\n",
      "zest\n",
      "schedule\n",
      "post\n",
      "dinner\n",
      "must\n",
      "afternoon\n",
      "mixed\n",
      "kyle\n",
      "within\n",
      "weather\n",
      "tiny\n",
      "your\n",
      "there\n",
      "hey\n",
      "foremo\n",
      "lonely\n",
      "forward\n",
      "head\n",
      "sizzle\n",
      "respectfu\n",
      "anywhere\n",
      "hear\n",
      "made\n",
      "mature\n",
      "clear\n",
      "nature\n",
      "moved\n",
      "lived\n",
      "judgemental\n",
      "maybe\n",
      "realize\n",
      "solano\n",
      "june\n",
      "fell\n",
      "miles\n",
      "wife\n",
      "age\n",
      "together\n",
      "gardens\n",
      "once\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "male\n",
      "artist\n",
      "yes\n",
      "know\n",
      "your\n",
      "filmmaker\n",
      "all\n",
      "answered\n",
      "very\n",
      "even\n",
      "let\n",
      "every\n",
      "love\n",
      "more\n",
      "beautiful\n",
      "want\n",
      "half\n",
      "not\n",
      "year\n",
      "little\n",
      "busy\n",
      "only\n",
      "watch\n",
      "sex\n",
      "please\n",
      "has\n",
      "middle\n",
      "romance\n",
      "bodies\n",
      "nylons\n",
      "questions\n",
      "anything\n",
      "ppl\n",
      "chat\n",
      "relationships\n",
      "send\n",
      "norm\n",
      "brown\n",
      "level\n",
      "50ies\n",
      "grandfather\n",
      "says\n",
      "full\n",
      "exchange\n",
      "along\n",
      "fantasies\n",
      "great\n",
      "smoke\n",
      "bloomer\n",
      "extra\n",
      "txt\n",
      "attenti\n",
      "live\n",
      "call\n",
      "iphone\n",
      "females\n",
      "work\n",
      "learn\n",
      "attract\n",
      "intelligent\n",
      "man\n",
      "light\n",
      "inordinary\n",
      "help\n",
      "soon\n",
      "perfect\n",
      "thank\n",
      "nostalgia\n",
      "late\n",
      "mail\n",
      "might\n",
      "into\n",
      "eventually\n",
      "seeking\n",
      "discuss\n",
      "belts\n",
      "each\n",
      "weight\n",
      "our\n",
      "sane\n",
      "size\n",
      "assured\n",
      "guess\n",
      "reason\n",
      "put\n",
      "beginning\n",
      "could\n",
      "days\n",
      "unhealthy\n",
      "feel\n",
      "number\n",
      "one\n",
      "creagive\n",
      "mist\n",
      "infatuation\n",
      "approximately\n",
      "eyes\n",
      "copy\n",
      "photograph\n",
      "boring\n",
      "hispanic\n",
      "were\n",
      "gays\n",
      "spurt\n",
      "youre\n",
      "well\n",
      "ages\n",
      "comfortable\n",
      "any\n",
      "able\n",
      "note\n",
      "equipment\n",
      "sure\n",
      "though\n",
      "most\n",
      "queens\n",
      "don\n",
      "artistic\n",
      "clean\n",
      "pictures\n",
      "show\n",
      "volunteer\n",
      "earth\n",
      "title\n",
      "black\n",
      "pretty\n",
      "personality\n",
      "watching\n",
      "geek\n",
      "married\n",
      "jokes\n",
      "culture\n",
      "best\n",
      "recently\n",
      "missing\n",
      "wear\n",
      "many\n",
      "amazing\n",
      "bronx\n",
      "whatever\n",
      "non\n",
      "laugh\n",
      "better\n",
      "women\n",
      "eastern\n",
      "crack\n",
      "else\n",
      "opposites\n",
      "chill\n",
      "these\n",
      "evaluated\n",
      "heels\n",
      "same\n",
      "exhibitionists\n",
      "drink\n",
      "totally\n",
      "cheer\n",
      "contact\n",
      "thanks\n",
      "m4w\n",
      "day\n",
      "other\n",
      "attractive\n",
      "m4m\n",
      "east\n",
      "take\n",
      "lacking\n",
      "couple\n",
      "possible\n",
      "dark\n",
      "possibly\n",
      "using\n",
      "lady\n",
      "projects\n",
      "shapes\n",
      "output\n",
      "page\n",
      "skinned\n",
      "passion\n",
      "170lb\n",
      "post\n",
      "plus\n",
      "own\n",
      "down\n",
      "involvement\n",
      "female\n",
      "area\n",
      "start\n",
      "hwp\n",
      "hear\n",
      "nassau\n",
      "flirt\n",
      "clear\n",
      "garter\n",
      "pic\n",
      "sticky\n",
      "film\n",
      "when\n",
      "lately\n",
      "really\n",
      "nice\n",
      "serious\n",
      "friends\n",
      "lead\n",
      "age\n",
      "admiring\n",
      "fresh\n",
      "once\n",
      "chinese\n",
      "relieve\n",
      "month\n",
      "commute\n",
      "10456\n",
      "children\n",
      "young\n",
      "mistakes\n",
      "finally\n",
      "rhythm\n",
      "smile\n",
      "include\n",
      "friendly\n",
      "vers\n",
      "woman\n",
      "mainstream\n",
      "advantage\n",
      "putting\n",
      "fatigued\n",
      "cool\n",
      "having\n",
      "race\n",
      "guy\n",
      "asian\n",
      "enjoy\n",
      "jamaica\n",
      "tired\n",
      "across\n",
      "video\n",
      "traveling\n",
      "link\n",
      "odd\n",
      "what\n",
      "stressful\n",
      "giving\n",
      "deeply\n",
      "indian\n",
      "learned\n",
      "body\n",
      "never\n",
      "here\n",
      "appeara\n",
      "met\n",
      "others\n",
      "alone\n",
      "kids\n",
      "healthy\n",
      "shift\n",
      "appointments\n",
      "experience\n",
      "museums\n",
      "jogging\n",
      "social\n",
      "usually\n",
      "rafael\n",
      "members\n",
      "family\n",
      "clara\n",
      "county\n",
      "ask\n",
      "use\n",
      "indians\n",
      "dude\n",
      "two\n",
      "symphony\n",
      "type\n",
      "tell\n",
      "relax\n",
      "club\n",
      "company\n",
      "baby\n",
      "insights\n",
      "join\n",
      "room\n",
      "hour\n",
      "movies\n",
      "cuddle\n",
      "dirnks\n",
      "give\n",
      "share\n",
      "hilton\n",
      "something\n",
      "sense\n",
      "pond\n",
      "swedish\n",
      "how\n",
      "sunday\n",
      "occasionally\n",
      "nyc\n",
      "tension\n",
      "staying\n",
      "funny\n",
      "may\n",
      "after\n",
      "date\n",
      "salsa\n",
      "lifting\n",
      "short\n",
      "natural\n",
      "waist\n",
      "think\n",
      "responsibility\n",
      "sexless\n",
      "pleasure\n",
      "playing\n",
      "talk\n",
      "minded\n",
      "exclusive\n",
      "furnished\n",
      "over\n",
      "move\n",
      "sooo\n",
      "years\n",
      "disabled\n",
      "committed\n",
      "hell\n",
      "still\n",
      "before\n",
      "anonymously\n",
      "fit\n",
      "willing\n",
      "them\n",
      "combination\n",
      "auto\n",
      "practice\n",
      "veggie\n",
      "lunch\n",
      "term\n",
      "name\n",
      "opera\n",
      "always\n",
      "pakistanis\n",
      "bless\n",
      "raised\n",
      "side\n",
      "everyone\n",
      "house\n",
      "fish\n",
      "idea\n",
      "bodywork\n",
      "sexual\n",
      "saturday\n",
      "special\n",
      "troll\n",
      "living\n",
      "since\n",
      "farm\n",
      "seriously\n",
      "got\n",
      "free\n",
      "california\n",
      "york\n",
      "meetups\n",
      "companion\n",
      "sbm\n",
      "keep\n",
      "silicon\n",
      "conversation\n",
      "american\n",
      "moving\n",
      "instruc\n",
      "south\n",
      "first\n",
      "relate\n",
      "dp_patron\n",
      "lol\n",
      "another\n",
      "open\n",
      "170\n",
      "top\n",
      "accent\n",
      "weekdays\n",
      "anyone\n",
      "pains\n",
      "185\n",
      "white\n",
      "interests\n",
      "sebastopol\n",
      "way\n",
      "relationship\n",
      "cup\n",
      "brushes\n",
      "than\n",
      "remember\n",
      "std\n",
      "grew\n",
      "depressed\n",
      "project\n",
      "marriage\n",
      "feeling\n",
      "lbs\n",
      "san\n",
      "mine\n",
      "nonfiction\n",
      "need\n",
      "aged\n",
      "chatting\n",
      "dancing\n",
      "without\n",
      "build\n",
      "which\n",
      "green\n",
      "latino\n",
      "hello\n",
      "singles\n",
      "reach\n",
      "embarcadero\n",
      "regular\n",
      "experiences\n",
      "plan\n",
      "significant\n",
      "elledge\n",
      "nothing\n",
      "plays\n",
      "average\n",
      "sometimes\n",
      "dog\n",
      "tech\n",
      "professional\n",
      "saying\n",
      "text\n",
      "meetings\n",
      "rosa\n",
      "therapist\n",
      "queer\n",
      "find\n",
      "pick\n",
      "advanced\n",
      "writer\n",
      "should\n",
      "being\n",
      "employed\n",
      "circle\n",
      "interested\n",
      "hope\n",
      "hopefully\n",
      "theft\n",
      "reply\n",
      "considerate\n",
      "falling\n",
      "endless\n",
      "wiccan\n",
      "sustainably\n",
      "seldom\n",
      "bay\n",
      "naked\n",
      "stuff\n",
      "discu\n",
      "hike\n",
      "incarcerated\n",
      "see\n",
      "bare\n",
      "wonder\n",
      "movie\n",
      "currently\n",
      "enough\n",
      "between\n",
      "probably\n",
      "enjoys\n",
      "reading\n",
      "email\n",
      "available\n",
      "comp\n",
      "attention\n",
      "however\n",
      "job\n",
      "preferably\n",
      "coffee\n",
      "come\n",
      "both\n",
      "last\n",
      "invited\n",
      "barely\n",
      "etc\n",
      "games\n",
      "dated\n",
      "lookin\n",
      "kinds\n",
      "reside\n",
      "dinning\n",
      "relaxation\n",
      "height\n",
      "active\n",
      "caring\n",
      "decent\n",
      "bbc\n",
      "due\n",
      "been\n",
      "much\n",
      "woke\n",
      "board\n",
      "life\n",
      "educated\n",
      "gay\n",
      "mind\n",
      "gal\n",
      "deep\n",
      "ming\n",
      "sound\n",
      "myself\n",
      "look\n",
      "straight\n",
      "chinatown\n",
      "will\n",
      "involved\n",
      "fun\n",
      "taking\n",
      "ive\n",
      "hoping\n",
      "almost\n",
      "site\n",
      "pinky\n",
      "cant\n",
      "tissue\n",
      "partner\n",
      "aches\n",
      "develop\n",
      "things\n",
      "make\n",
      "fidi\n",
      "grand\n",
      "party\n",
      "gets\n",
      "8am\n",
      "status\n",
      "meets\n",
      "used\n",
      "pediatric\n",
      "banter\n",
      "weed\n",
      "kik\n",
      "heartbroken\n",
      "running\n",
      "arrived\n",
      "expand\n",
      "off\n",
      "matters\n",
      "24yr\n",
      "chickens\n",
      "customized\n",
      "person\n",
      "stressed\n",
      "172cm\n",
      "athletic\n",
      "money\n",
      "yrs\n",
      "shape\n",
      "bangladeshis\n",
      "yet\n",
      "hiking\n",
      "also\n",
      "marrying\n",
      "drive\n",
      "had\n",
      "lets\n",
      "easter\n",
      "kick\n",
      "real\n",
      "lubed\n",
      "around\n",
      "wanting\n",
      "bit\n",
      "walk\n",
      "rotate\n",
      "lost\n",
      "night\n",
      "therapeutic\n",
      "people\n",
      "spring\n",
      "pals\n",
      "some\n",
      "back\n",
      "born\n",
      "clubs\n",
      "home\n",
      "caribbean\n",
      "normal\n",
      "creative\n",
      "everything\n",
      "does\n",
      "benefitted\n",
      "zest\n",
      "schedule\n",
      "fitness\n",
      "santa\n",
      "session\n",
      "actual\n",
      "working\n",
      "getting\n",
      "reduced\n",
      "drama\n",
      "dinner\n",
      "must\n",
      "afternoon\n",
      "mixed\n",
      "kyle\n",
      "ladies\n",
      "mommy\n",
      "within\n",
      "swed\n",
      "weather\n",
      "tiny\n",
      "her\n",
      "spending\n",
      "housing\n",
      "there\n",
      "hey\n",
      "long\n",
      "foremo\n",
      "lonely\n",
      "lot\n",
      "forward\n",
      "cultures\n",
      "valley\n",
      "was\n",
      "190lb\n",
      "head\n",
      "sizzle\n",
      "offering\n",
      "respectfu\n",
      "anywhere\n",
      "but\n",
      "flexible\n",
      "trying\n",
      "spice\n",
      "made\n",
      "mature\n",
      "similar\n",
      "nature\n",
      "butthole\n",
      "moved\n",
      "muscular\n",
      "single\n",
      "lived\n",
      "judgemental\n",
      "again\n",
      "maybe\n",
      "book\n",
      "manhattan\n",
      "realize\n",
      "picture\n",
      "solano\n",
      "june\n",
      "fell\n",
      "humor\n",
      "pool\n",
      "ago\n",
      "miles\n",
      "wife\n",
      "together\n",
      "loyal\n",
      "weights\n",
      "time\n",
      "smoker\n",
      "gardens\n",
      "fully\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny,sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
